{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikhilNamal17/engg-course-work/blob/master/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1V6ND4Iesp",
        "colab_type": "text"
      },
      "source": [
        "Tokenizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNFO0Xi5EQYX",
        "colab_type": "text"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt') \n",
        "  \n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle') \n",
        "  \n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmhQxZOAI8oM",
        "colab_type": "text"
      },
      "source": [
        "Stremmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXh-kcY6I5dX",
        "colab_type": "code",
        "outputId": "17d75db3-3360-4860-f7ce-c46ac669748e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "ps=nltk.PorterStemmer()\n",
        "words=[\"program\",\"programer\",\"programing\",\"programers\"]\n",
        "for a in words:\n",
        "    print(a,\":\",ps.stem(a))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program : program\n",
            "programer : program\n",
            "programing : program\n",
            "programers : program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjRsA5t2JJPE",
        "colab_type": "text"
      },
      "source": [
        "Stop Word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekkkf0p4JMAM",
        "colab_type": "code",
        "outputId": "56d2e2bd-2443-4bda-ddd9-31157ebc5523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')  \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"  \n",
        "stop_words = set(stopwords.words('english'))   \n",
        "word_tokens = word_tokenize(example_sent)   \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "filtered_sentence = []   \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w)\n",
        "print(word_tokens) \n",
        "print(filtered_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckxm0XynJT9d",
        "colab_type": "text"
      },
      "source": [
        "Lemmatizer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7AKjYf0JW3F",
        "colab_type": "code",
        "outputId": "008c73b1-a342-4805-c8f0-70f3da43e808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "lemmatizer = WordNetLemmatizer()   \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))   \n",
        "# a denotes adjective in \"pos\" \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9waRrI8AJ0TZ",
        "colab_type": "text"
      },
      "source": [
        "N gram analyser: 3rd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4av1SZhJ1K3",
        "colab_type": "code",
        "outputId": "1d4ab5f6-5a31-4b11-eef0-eef4aa41aea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "data = 'A class is a blueprint for the object.'\n",
        "print(\"1-gram: \", extract_ngrams(data, 1))\n",
        "print(\"2-gram: \", extract_ngrams(data, 2))\n",
        "print(\"3-gram: \", extract_ngrams(data, 3))\n",
        "print(\"4-gram: \", extract_ngrams(data, 4))\n",
        "print(\"6-gram: \", extract_ngrams(data, 6))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n",
            "6-gram:  ['A class is a blueprint for', 'class is a blueprint for the', 'is a blueprint for the object', 'a blueprint for the object .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ9OexiBKb_5",
        "colab_type": "code",
        "outputId": "e907f295-593c-44e7-c3d1-fe6fb8ee1d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "#Dummy text\n",
        "txt = input(\"enter the text\")\n",
        "'''txt = \"Sukanya, Rajib and Naba are my good friends. \"  \n",
        "    \"Sukanya is getting married next year. \" \\ \n",
        "    \"Marriage is a big step in oneâ€™s life.\" \\ \n",
        "    \"It is both exciting and frightening. \" \\ \n",
        "    \"But friendship is a sacred bond between people.\" \\ \n",
        "    \"It is a special kind of love between us. \" \\ \n",
        "    \"Many of you must have tried searching for a friend \"\\ \n",
        "    \"but never found the right one.\"'''\n",
        "  \n",
        "# sent_tokenize is one of instances of  \n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
        "tokenized = sent_tokenize(txt) \n",
        "for i in tokenized: \n",
        "      \n",
        "    # Word tokenizers is used to find the words  \n",
        "    # and punctuation in a string \n",
        "    wordsList = nltk.word_tokenize(i) \n",
        "    # removing stop words from wordList \n",
        "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
        "    #  Using a Tagger. Which is part-of-speech  \n",
        "    # tagger or POS-tagger.  \n",
        "    tagged = nltk.pos_tag(wordsList) \n",
        "    print(txt)\n",
        "    print(tagged)\t\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "enter the textNikhil, Raj and Naba are my good friends\n",
            "Nikhil, Raj and Naba are my good friends\n",
            "[('Nikhil', 'NNP'), (',', ','), ('Raj', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz_W07WSL5lo",
        "colab_type": "code",
        "outputId": "211b2dcc-9a9f-47a9-f1f9-74849e74b4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "text =\"learn php from guru99 and make study easy\".split()\n",
        "print(\"After Split:\",text)\n",
        "tokens_tag = pos_tag(text)\n",
        "print(\"After Token:\",tokens_tag)\n",
        "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "print(\"After Regex:\",chunker)\n",
        "output = chunker.parse(tokens_tag)\n",
        "print(\"After Chunking\",output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
            "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
            "After Regex: chunk.RegexpParser with 1 stages:\n",
            "RegexpChunkParser with 1 rules:\n",
            "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
            "After Chunking (S\n",
            "  (mychunk learn/JJ)\n",
            "  (mychunk php/NN)\n",
            "  from/IN\n",
            "  (mychunk guru99/NN and/CC)\n",
            "  make/VB\n",
            "  (mychunk study/NN easy/JJ))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}